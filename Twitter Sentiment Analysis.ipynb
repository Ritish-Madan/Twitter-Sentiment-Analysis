{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the document with pandas reader\n",
    "train = pd.read_csv('twitter analysis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We basically need to analyse the sentiments, so we do not require the other content except the tweets and their sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_train_data = train[[\"airline_sentiment\", \"text\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering the required data in tuple form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format = []\n",
    "tweets = required_train_data['text']\n",
    "sentiments = required_train_data['airline_sentiment']\n",
    "for i in range(len(required_train_data)):\n",
    "    data_format.append((word_tokenize(tweets[i]), sentiments[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the usernames from the tweet\n",
    "We need to remove the usernames from the tweets, as the Usernames must not be a factor of a comment, but the tweet content should be. So, Most of the usernames are present in the starting, so I removed the starting trailing usernames, there are very few usernames which are in between the tweets, so they will be automatically filtered away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in data_format:\n",
    "    i = 0\n",
    "    while(i < len(j[0])):\n",
    "        if(j[0][0] == \"@\"):\n",
    "            j[0].pop(0)\n",
    "            j[0].pop(0)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the cleaned data(Cleaned Data with usernames)\n",
    "partial_clean = data_format.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "punctuations = list(string.punctuation)\n",
    "stops = stop_words + punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Major Data Cleaning\n",
    "Cleaning the data includes:\n",
    "1. Removind the stop words\n",
    "2. Lemmatize the data with WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get POS as the argument ready for WordNetLemmatizer function attribute lemmatize\n",
    "def get_simple_pos(tag):\n",
    "    if(tag.startswith('J')):\n",
    "        return wordnet.ADJ\n",
    "    elif(tag.startswith('V')):\n",
    "        return wordnet.VERB\n",
    "    elif(tag.startswith('N')):\n",
    "        return wordnet.NOUN\n",
    "    elif(tag.startswith('R')):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning functions, that checks the top words, lemmatize, and check if the word is alphabet or not. We do not include the\n",
    "# digits and special characters.\n",
    "\n",
    "def clean(words):\n",
    "    output = [] # Storing the output cleaned words in the list\n",
    "    for w in words: # Iterating thorugh each word\n",
    "        if(w.lower() not in stops and w.isalpha() and w.lower() != \"aa\"): # Lowercase the word to check in stopwords and including only alphabets\n",
    "            pos = pos_tag([w]) # Part of Speech\n",
    "            clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][0])) # Lemmatizing the words, WordNetLemmatizer\n",
    "            output.append(clean_word.lower()) # Storing the output\n",
    "    return output # Returning cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = [(clean(text),sentiment) for text, sentiment in partial_clean] # Passing the data to above clean function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great now, we have got our data cleaned up and ready to train. Let's split our data into training and testing so that we can test our data on the different prediction algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = [sentiment for text, sentiment in cleaned_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\" \".join(tweet) for tweet, sentiment in cleaned_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tweets, sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our data\n",
    "As of now we have cleaned our data, split that into two parts (training and testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer library\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVector = CountVectorizer(max_features = 2000, max_df = 0.8, ngram_range = (1, 2))\n",
    "X_train = countVector.fit_transform(x_train)\n",
    "X_test = countVector.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Random Forests Classifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=3, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C = 3)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7712204007285974"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# np.savetxt('outputs.csv', result, fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
